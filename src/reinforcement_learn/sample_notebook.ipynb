{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d8c406",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning (RL) is a foundational area of machine learning where an agent learns to make decisions by interacting with an environment to achieve a goal. RL is distinguished by how it mimics the trial-and-error learning process humans and animals use to acquire new skills. Given its roots in behavioral psychology and decision theory, RL is crucial for developing autonomous systems that evolve and adapt over time.\n",
    "\n",
    "At its core, RL involves:\n",
    "\n",
    "- **Agents**: The learner or decision-maker.\n",
    "- **Environment**: The external system with which the agent interacts.\n",
    "- **Actions**: All possible moves the agent can take.\n",
    "- **States**: Current situations returned by the environment.\n",
    "- **Rewards**: Feedback from the environment for each action.\n",
    "\n",
    "This notebook explores various methodologies within RL, demonstrating their theory, common applications, and practical implementations in Python.\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "### Theory\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making wherein outcomes are partly random and partly under the control of a decision-maker.\n",
    "\n",
    "**MDP Components:**\n",
    "\n",
    "- **States (S):** A finite set representing the situations the agent can encounter.\n",
    "- **Actions (A):** A finite set of possible actions the agent can take.\n",
    "- **Transition Model (T):** Probability distribution over states guiding state transition after taking an action.\n",
    "- **Reward Function (R):** Immediate reward received after transitioning from one state to another.\n",
    "- **Policy (π):** A strategy that specifies the action that the agent takes in each state.\n",
    "\n",
    "The objective in an MDP is to find a policy π that maximizes the expected sum of rewards (also known as the return).\n",
    "\n",
    "### Examples\n",
    "\n",
    "Let's consider a simple grid world where an agent can move in four directions: up, down, left, and right until it reaches a goal or falls into a pit.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, grid, start, terminal_states, rewards):\n",
    "        self.grid = grid\n",
    "        self.agent_position = start\n",
    "        self.terminal_states = terminal_states\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == \"up\" and self.agent_position[0] > 0:\n",
    "            self.agent_position[0] -= 1\n",
    "        elif action == \"down\" and self.agent_position[0] < len(self.grid)-1:\n",
    "            self.agent_position[0] += 1\n",
    "        elif action == \"left\" and self.agent_position[1] > 0:\n",
    "            self.agent_position[1] -= 1\n",
    "        elif action == \"right\" and self.agent_position[1] < len(self.grid[0])-1:\n",
    "            self.agent_position[1] += 1\n",
    "\n",
    "        reward = self.rewards.get(tuple(self.agent_position), 0)\n",
    "        done = tuple(self.agent_position) in self.terminal_states\n",
    "        return tuple(self.agent_position), reward, done\n",
    "\n",
    "# Define the grid, starting point, terminal states, and rewards\n",
    "grid = np.zeros((4, 4))\n",
    "start = [0, 0]\n",
    "terminal_states = [(3, 3), (3, 1)]\n",
    "rewards = {(3, 3): 1, (3, 1): -1}\n",
    "\n",
    "# Initialize and interact with the environment\n",
    "env = GridWorld(grid, start, terminal_states, rewards)\n",
    "\n",
    "# Sample interaction\n",
    "action = random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
    "state, reward, done = env.step(action)\n",
    "print(f\"Action: {action}, New State: {state}, Reward: {reward}, Done: {done}\")\n",
    "```\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "### Theory\n",
    "\n",
    "Q-Learning is an off-policy, model-free RL algorithm to learn the value of an action in a particular state. It seeks to find the best policy by learning an action-value function, \\( Q(s, a) \\), representing the expected utility of taking a given action \\( a \\) in a given state \\( s \\).\n",
    "\n",
    "**Q-Learning Update Rule**:\n",
    "\n",
    "\\[\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right]\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( \\alpha \\) is the learning rate.\n",
    "- \\( \\gamma \\) is the discount factor.\n",
    "- \\( r \\) is the reward observed after executing action \\( a \\).\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, exploration_decay=0.99, exploration_min=0.1):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_min = exploration_min\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return random.choice(range(self.action_size))\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate * self.exploration_decay)\n",
    "\n",
    "# Usage:\n",
    "num_states = 4 * 4  # Example state size for a 4x4 grid\n",
    "num_actions = 4  # Up, down, left, right\n",
    "agent = QLearningAgent(num_states, num_actions)\n",
    "\n",
    "# Simulate interactions with the environment\n",
    "state = 0  # Initial state\n",
    "next_state = 1  # Example state transition\n",
    "reward = 0  # Sample reward\n",
    "\n",
    "action = agent.choose_action(state)\n",
    "agent.update(state, action, reward, next_state)\n",
    "```\n",
    "\n",
    "## Policy Gradient Methods\n",
    "\n",
    "### Theory\n",
    "\n",
    "Policy Gradient Methods directly learn the policy which maps states to actions without needing a value function. Policies are usually parameterized with a neural network, and learning involves optimizing these parameters to maximize expected return.\n",
    "\n",
    "**Objective Function**:\n",
    "\n",
    "\\[\n",
    "J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]\n",
    "\\]\n",
    "\n",
    "**Gradient Ascent**:\n",
    "The parameters \\( \\theta \\) are updated using the gradient ascent:\n",
    "\n",
    "\\[\n",
    "\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\]\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(state_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "class PolicyGradientAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.01):\n",
    "        self.policy_network = PolicyNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        probs = self.policy_network(state)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def update(self, rewards, log_probs):\n",
    "        discounts = np.array([0.99**i for i in range(len(rewards))])\n",
    "        returns = np.array([sum(rewards[i:] * discounts[:len(rewards[i:])]) for i in range(len(rewards))])\n",
    "        returns = torch.FloatTensor(returns)\n",
    "\n",
    "        loss = []\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            loss.append(-log_prob * G)\n",
    "        loss = torch.cat(loss).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Simulating an environment interaction\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "agent = PolicyGradientAgent(state_size, action_size)\n",
    "\n",
    "state = np.random.rand(state_size)  # Example state\n",
    "action = agent.choose_action(state)\n",
    "\n",
    "# Assume we collected rewards and log_probs during an episode\n",
    "rewards = [1, 0, -1]\n",
    "log_probs = [torch.log(torch.tensor(0.5)), torch.log(torch.tensor(0.3)), torch.log(torch.tensor(0.8))]\n",
    "agent.update(rewards, log_probs)\n",
    "```\n",
    "\n",
    "## Deep Reinforcement Learning\n",
    "\n",
    "### Theory\n",
    "\n",
    "Deep Reinforcement Learning (Deep RL) combines neural networks with reinforcement learning principles to create end-to-end systems where the agent learns from high-dimensional inputs. The incorporation of deep learning enables RL to tackle problems with large state spaces, such as visual inputs from games.\n",
    "\n",
    "A popular Deep RL algorithm is Deep Q-Networks (DQN), which approximates the Q-function using a neural network:\n",
    "\n",
    "- **Experience Replay**: Stores past experiences and samples a mini-batch for training to break the correlation between consecutive samples.\n",
    "- **Target Network**: A separate network to calculate target Q-values that is periodically updated to stabilize learning.\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size=64, gamma=0.99, lr=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.target_model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.update_target()\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > 2000:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < 0.1:  # Exploration vs Explotation\n",
    "            return random.choice(range(self.action_size))\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.amax(self.target_model(next_state)).item()\n",
    "\n",
    "            q_values = self.model(state)\n",
    "            q_value = q_values[0][action]\n",
    "\n",
    "            loss = nn.functional.mse_loss(q_value, torch.tensor(target))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "# Usage in a hypothetical environment\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Remember past experiences ...\n",
    "agent.remember([1, 0, 0, 1], 1, 1, [1, 1, 0, 0], False)\n",
    "\n",
    "# ... and learn from them\n",
    "agent.replay()\n",
    "```\n",
    "\n",
    "## Applications in Game AI\n",
    "\n",
    "### Theory\n",
    "\n",
    "Reinforcement Learning has extensive applications in Game AI, where it’s used to develop strategies that outperform traditional rule-based methods. Notable successes include AlphaGo's defeat of the world champion in the game of Go, using strategies that combined RL with deep neural networks and tree search techniques.\n",
    "\n",
    "In video games, RL systems can learn to play directly from pixels, surpassing human-level performance in several cases. This success is exemplified by breakthroughs such as DeepMind's accomplishment in Atari games using the DQN algorithm.\n",
    "\n",
    "### Examples\n",
    "\n",
    "The use of RL in Game AI generally involves training an agent using extensive simulated game play. Here's a very simplified setup for a Tic-Tac-Toe game:\n",
    "\n",
    "```python\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [' '] * 9\n",
    "        self.current_winner = None\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [i for i, spot in enumerate(self.board) if spot == ' ']\n",
    "    \n",
    "    def make_move(self, square, letter):\n",
    "        if self.board[square] == ' ':\n",
    "            self.board[square] = letter\n",
    "            if self.winner(square, letter):\n",
    "                self.current_winner = letter\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def winner(self, square, letter):\n",
    "        # Check if the combination leads to a win\n",
    "        win_conditions = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "            [0, 4, 8], [2, 4, 6]\n",
    "        ]\n",
    "        for condition in win_conditions:\n",
    "            if all([self.board[i] == letter for i in condition]):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Simple example to use RL for Tic-Tac-Toe\n",
    "game = TicTacToe()\n",
    "agent_x = QLearningAgent(9, 9)\n",
    "agent_o = QLearningAgent(9, 9)  # Agent for playing 'O'\n",
    "\n",
    "# Each agent learns to play the game through repeated simulations\n",
    "for episode in range(10000):\n",
    "    current_player = 'X'\n",
    "\n",
    "    while not game.current_winner:\n",
    "        available = game.available_moves()\n",
    "        if current_player == 'X':\n",
    "            action = agent_x.choose_action(len(available))\n",
    "            game.make_move(action, current_player)\n",
    "            current_player = 'O'\n",
    "        else:\n",
    "            action = agent_o.choose_action(len(available))\n",
    "            game.make_move(action, current_player)\n",
    "            current_player = 'X'\n",
    "    \n",
    "    # Update the agents based on game outcomes...\n",
    "\n",
    "print(\"Training complete!\")\n",
    "```\n",
    "\n",
    "In these applications, RL algorithms adapt through self-play, ultimately creating complex AI models that behave optimally or creatively in game environments."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
