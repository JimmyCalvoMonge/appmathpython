{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a9aaba",
   "metadata": {},
   "source": [
    "# Clustering Theory in Data Science\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Clustering is a fundamental aspect of unsupervised machine learning, utilized for grouping a set of objects in such a way that objects in the same group (or cluster) share more similarities compared to those in other groups. It does not require labeled data and is widely used for pattern recognition, data analysis, and information retrieval. In this notebook, we will explore different clustering methods, including K-Means Clustering, Hierarchical Clustering, Density-Based Clustering, and Model-Based Clustering. Furthermore, we will discuss various techniques for Cluster Validation, ensuring that the clusters formed provide meaningful insights.\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "### Theory\n",
    "\n",
    "K-Means Clustering is a partitioning method that divides a dataset into `k` distinct non-overlapping subgroups (clusters). The objective is to minimize the variance within each cluster. The algorithm follows these steps:\n",
    "1. Initialize `k` centroids randomly.\n",
    "2. Assign each data point to the nearest centroid.\n",
    "3. Update centroids as the mean of all data points assigned to that centroid's cluster.\n",
    "4. Repeat steps 2 and 3 until convergence.\n",
    "\n",
    "Mathematically, the aim is to minimize the following cost function:\n",
    "\n",
    "\\[ J = \\sum_{i=1}^{k} \\sum_{x \\in S_i} \\|x - \\mu_i\\|^2 \\]\n",
    "\n",
    "where \\(S_i\\) is the set of points in cluster \\(i\\) and \\(\\mu_i\\) is the centroid of cluster \\(i\\). The Euclidean distance is commonly used as the distance metric.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Hereâ€™s an implementation of K-Means Clustering in Python using the `scikit-learn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some data\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# Create KMeans object and fit to data\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "# Print cluster centers and labels\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n",
    "print(\"Labels:\\n\", kmeans.labels_)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Customer segmentation based on purchasing behavior\n",
    "- Image compression by reducing the number of colors\n",
    "- Anomaly detection by identifying clusters of unusual patterns\n",
    "\n",
    "## Hierarchical Clustering\n",
    "\n",
    "### Theory\n",
    "\n",
    "Hierarchical Clustering seeks to build a hierarchy of clusters and can be executed in two modes: Agglomerative (bottom-up) and Divisive (top-down). The agglomerative method starts with individual data points (each being a cluster) and merges them until a single cluster is formed, whereas the divisive approach starts with the whole dataset and continuously splits it.\n",
    "\n",
    "Agglomerative clustering uses a linkage criterion to determine which clusters to merge at each step:\n",
    "- **Single linkage**: Minimum distance between points in different clusters.\n",
    "- **Complete linkage**: Maximum distance between points in different clusters.\n",
    "- **Average linkage**: Average distance between all points in different clusters.\n",
    "\n",
    "The result can be visualized using a dendrogram, which shows the arrangements of the clusters.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Using `scipy` to perform Hierarchical Clustering:\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(X, 'single')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, \n",
    "           orientation='top',\n",
    "           distance_sort='descending',\n",
    "           show_leaf_counts=True)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Establishing taxonomies in biology by grouping species\n",
    "- Organizing documents into a hierarchy for information retrieval\n",
    "- Social network analysis by detecting community structures\n",
    "\n",
    "## Density-Based Clustering\n",
    "\n",
    "### Theory\n",
    "\n",
    "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a density-based clustering algorithm. It defines clusters as dense regions of points, separated by sparse regions (noise):\n",
    "\n",
    "- **Eps-neighborhood**: The region within distance \\(\\epsilon\\) from a point.\n",
    "- **Core point**: A point with at least `minPts` neighbors in its Eps-neighborhood.\n",
    "- **Border point**: A point that is not a core point but falls within the Eps-neighborhood of a core point.\n",
    "- **Noise point**: A point that is neither a core point nor a border point.\n",
    "\n",
    "DBSCAN can identify clusters of varying shapes and sizes and can manage noise effectively.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Implementing DBSCAN using `scikit-learn`:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [1, 3], [1, 4], [2, 2], [8, 7], [8, 8], [25, 80]])\n",
    "\n",
    "# DBSCAN fitting\n",
    "db = DBSCAN(eps=3, min_samples=2).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "# Visualize DBSCAN clustering\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Geographic clustering of data points such as earthquakes\n",
    "- Anomaly detection in systems, identifying clusters of unusual patterns\n",
    "- Market analysis by detecting user behavior activity regions\n",
    "\n",
    "## Model-Based Clustering\n",
    "\n",
    "### Theory\n",
    "\n",
    "Model-Based Clustering assumes data is generated from a mixture of probability distributions, typically Gaussian. Each cluster corresponds to a different distribution. The Expectation-Maximization (EM) algorithm is commonly used for finding the Maximum Likelihood estimates of parameters in these models. \n",
    "\n",
    "- **Expectation step**: Compute the probability of each data point belonging to each cluster.\n",
    "- **Maximization step**: Update the parameters of the distributions to maximize these probabilities.\n",
    "\n",
    "This method is powerful for identifying intricate structures in data where traditional clustering might fail.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Using Gaussian Mixture Models in `scikit-learn`:\n",
    "\n",
    "```python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[0.1, 0.2], [1.8, 1.5], [1.0, 3.1], [6.5, 7.8], [4.5, 4.8]])\n",
    "\n",
    "# Fit Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=2, random_state=0).fit(X)\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "# Visualize GMM clustering\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Gaussian Mixture Model Clustering\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Market segmentation by identifying distinct customer profiles\n",
    "- Biostatistics for clustering gene expression data\n",
    "- Learning and discovering hidden structures in data\n",
    "\n",
    "## Cluster Validation\n",
    "\n",
    "Cluster Validation is essential to ensure the quality and validity of the clusters formed. Tools for validation can be categorized into internal, external, and relative indices:\n",
    "\n",
    "### Theory\n",
    "\n",
    "- **Internal validation**: Uses internal information to assess the quality of the clustering without reference to external data. Common measures include Silhouette Score, Dunn Index, etc.\n",
    "- **External validation**: Compares the clustering results to a pre-defined ground truth. Measures include Adjusted Rand Index, Normalized Mutual Information, etc.\n",
    "- **Relative validation**: Compares different clustering models to choose the best one, often using methods like Elbow Method, Gap Statistics, Cross Validation.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Using Silhouette Score in `scikit-learn` which measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation):\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# Fit K-Means\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Calculate silhouette score\n",
    "score = silhouette_score(X, labels, metric='euclidean')\n",
    "print(\"Silhouette Score:\", score)\n",
    "```\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Evaluate the effectiveness of different clustering algorithms on specific datasets\n",
    "- Determine the best number of clusters for K-Means using the Elbow Method or Silhouette Analysis\n",
    "- Validate clusters by ensuring meaningful interpretation and prediction of results\n",
    "\n",
    "In conclusion, understanding and selecting the appropriate clustering method along with effective validation techniques play a critical role in gaining meaningful insights from data. Each clustering technique presents its unique advantages, thus comprehending their theoretical underpinnings and practical applications ensures that one can choose suitably based on the nature of the problem and dataset at hand.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
