{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c590b7a",
   "metadata": {},
   "source": [
    "# Classification Models in Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Classification models are an integral part of machine learning used to categorize data into predefined classes. Unlike regression models which predict continuous outputs, classification models focus on predicting discrete outcomes. These models have wide applications across different fields, from medical diagnosis to spam detection. In this notebook, we will delve into some of the popular classification models, exploring their theoretical underpinnings, potential applications, and practical implementation in Python.\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "### Theory\n",
    "\n",
    "Decision trees are a non-parametric supervised learning method used for classification (and regression). The fundamental idea is to split the data into subsets based on the value of input features, constructing a tree with decision nodes and leaf nodes. Each internal node represents a \"test\" on an attribute, each branch a result of the test, and each leaf node a class label.\n",
    "\n",
    "The decision tree can be represented mathematically through concepts like entropy or Gini impurity. The goal is to maximize information gain, defined as:\n",
    "\n",
    "\\[\n",
    "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\cdot Entropy(S_v)\n",
    "\\]\n",
    "\n",
    "Where \\(Entropy(S)\\) is calculated as:\n",
    "\n",
    "\\[\n",
    "Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "### Examples\n",
    "\n",
    "Here is a simple Python example using scikit-learn to classify a dataset with a decision tree:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train model\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "```\n",
    "\n",
    "**Real Applications:** Decision trees are used in credit scoring, diagnosis of diseases, and even recommendation systems.\n",
    "\n",
    "## Support Vector Machines\n",
    "\n",
    "### Theory\n",
    "\n",
    "Support Vector Machines (SVM) are powerful classification techniques that aim to find the hyperplane that best separates different classes in a feature space. The vector points lying on the hyperplane are called support vectors. For a linearly separable dataset, the decision function is:\n",
    "\n",
    "\\[\n",
    "f(x) = \\text{sign}(w \\cdot x + b)\n",
    "\\]\n",
    "\n",
    "where \\(w\\) is the weight vector and \\(b\\) is the bias. In nonlinear cases, kernel functions transform the feature space, enabling hyperplane separation.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Using SVM with a Radial Basis Function (RBF) kernel in Python:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm_clf = SVC(kernel='rbf', gamma='auto')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "```\n",
    "\n",
    "**Real Applications:** SVMs are used in image recognition, text categorization, and bioinformatics.\n",
    "\n",
    "## Neural Networks for Classification\n",
    "\n",
    "### Theory\n",
    "\n",
    "Neural Networks are computational models inspired by the human brain, consisting of interconnected layers of nodes called neurons. Each neuron computes a weighted sum of its inputs, applies an activation function, and passes the result to the next layer. The network is trained through optimization techniques like backpropagation to minimize a loss function.\n",
    "\n",
    "For a binary classification problem, the neural network might use the sigmoid activation function, modeled as:\n",
    "\n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "For multiple classes, softmax is commonly used in the output layer to map scores to probabilities.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Building a simple neural network using Keras for classification:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset and preprocessing\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = to_categorical(iris.target)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define neural network model\n",
    "model = Sequential([\n",
    "    Dense(10, input_shape=(4,), activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "**Real Applications:** Neural networks are widely used in image and voice recognition, natural language processing, and financial forecasting.\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "### Theory\n",
    "\n",
    "Ensemble Methods are techniques that combine multiple models to improve overall performance. The main idea is that multiple weak models, when combined, produce a more accurate prediction. Two popular ensemble methods are **Bagging** (Bootstrap Aggregating) and **Boosting**.\n",
    "\n",
    "**Bagging:** Each model is trained on a random subset of the data. Predictions from these models are combined, usually by voting.\n",
    "\n",
    "**Boosting:** Models are added sequentially, each correcting the errors of its predecessor. Popular boosting algorithms include AdaBoost and Gradient Boosting.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Here is how you can implement Random Forest (a bagging method) and AdaBoost using scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_accuracy = rf_clf.score(X_test, y_test)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "\n",
    "# AdaBoost\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_accuracy = ada_clf.score(X_test, y_test)\n",
    "print(f\"AdaBoost Accuracy: {ada_accuracy}\")\n",
    "```\n",
    "\n",
    "**Real Applications:** Ensemble methods excel in competition solutions, fraud detection, and in many winning algorithms in platforms like Kaggle.\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Theory\n",
    "\n",
    "Performance metrics are crucial for evaluating classification model effectiveness. Here are key metrics:\n",
    "\n",
    "- **Accuracy:** Ratio of correctly predicted observations to total observations.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  \\]\n",
    "\n",
    "- **Precision:** Ratio of correctly predicted positive observations to total predicted positives.\n",
    "\n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  \\]\n",
    "\n",
    "- **Recall (Sensitivity):** Ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  \\]\n",
    "\n",
    "- **F1 Score:** Harmonic mean of Precision and Recall, useful when the class distribution is uneven.\n",
    "\n",
    "  \\[\n",
    "  \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  \\]\n",
    "\n",
    "### Examples\n",
    "\n",
    "To compute these metrics using scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assuming y_test and y_pred from previous model predictions\n",
    "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "```\n",
    "\n",
    "In real-world scenarios, selecting the appropriate metric is critical and often depends on the importance of false positives versus false negatives within a specific domain."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
