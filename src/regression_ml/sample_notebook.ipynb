{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580301d5",
   "metadata": {},
   "source": [
    "# Regression Models in Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Regression analysis is a fundamental method in statistics and machine learning, allowing us to understand and model relationships between variables. By using regression, we can predict outcomes, identify trends, and make informed decisions. In this notebook, we will explore several common regression techniques, each having its particular uses and strengths, along with code examples and real-life applications to illustrate their power.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "### Theory\n",
    "\n",
    "Linear Regression is the simplest form of regression. It assumes a linear relationship between the independent variable (feature) \\( X \\) and the dependent variable (output) \\( Y \\). The relationship is modeled by the equation:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X + \\epsilon \\]\n",
    "\n",
    "where:\n",
    "- \\( Y \\) is the dependent variable.\n",
    "- \\( \\beta_0 \\) is the y-intercept.\n",
    "- \\( \\beta_1 \\) is the slope of the line.\n",
    "- \\( \\epsilon \\) represents the error term (residuals).\n",
    "\n",
    "The goal of Linear Regression is to find the parameters \\( \\beta_0 \\) and \\( \\beta_1 \\) that minimize the sum of squared residuals.\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "Y = np.array([2, 3, 5, 7, 11])\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, Y, color='blue', label=\"Data Points\")\n",
    "plt.plot(X, model.predict(X), color='red', label=\"Linear Fit\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression Example')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "*Real-world Application*: Predicting house prices based on various features such as size, location, and number of rooms.\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "### Theory\n",
    "\n",
    "Polynomial Regression is an extension of Linear Regression that models the relationship as an \\( n^{th} \\) degree polynomial. It is represented as:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\ldots + \\beta_nX^n + \\epsilon \\]\n",
    "\n",
    "This type of regression is useful for capturing non-linear patterns in the data.\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "Y = np.array([1, 4, 9, 16, 25])  # Quadratic relationship\n",
    "\n",
    "# Create a polynomial regression model\n",
    "degree = 2\n",
    "polynomial_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "polynomial_model.fit(X, Y)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, Y, color='green', label=\"Data Points\")\n",
    "plt.plot(X, polynomial_model.predict(X), color='orange', label=f\"Degree {degree} Polynomial Fit\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Polynomial Regression Example')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "*Real-world Application*: Modeling the growth of populations where growth rates change over time.\n",
    "\n",
    "## Ridge and Lasso Regression\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Ridge Regression** adds a penalty equivalent to the square of the magnitude of coefficients, which shrinks the coefficients and reduces model complexity.\n",
    "\n",
    "\\[ \\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum \\beta_j^2 \\]\n",
    "\n",
    "**Lasso Regression** adds a penalty equivalent to the absolute value of the magnitude of coefficients, which can shrink some coefficients to zero, allowing for feature selection.\n",
    "\n",
    "\\[ \\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum |\\beta_j| \\]\n",
    "\n",
    "The term \\( \\lambda \\) is a hyperparameter that controls the strength of the penalty.\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Example data\n",
    "X = np.random.rand(10, 3)\n",
    "Y = np.dot(X, np.array([1.5, -2, 3])) + np.random.normal(0, 0.1, 10)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X, Y)\n",
    "ridge_coef = ridge_model.coef_\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X, Y)\n",
    "lasso_coef = lasso_model.coef_\n",
    "\n",
    "print(f'Ridge Coefficients: {ridge_coef}')\n",
    "print(f'Lasso Coefficients: {lasso_coef}')\n",
    "```\n",
    "\n",
    "*Real-world Application*: Ridge is often used for multicollinear data where predictor variables are highly correlated, while Lasso is useful for reducing model size by selecting influential features.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### Theory\n",
    "\n",
    "Logistic Regression is used for binary classification problems. It predicts the probability of occurrence of an event by fitting data to a logistic curve. The logistic model is given by:\n",
    "\n",
    "\\[ P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X)}} \\]\n",
    "\n",
    "where \\( P(Y=1|X) \\) is the probability that the dependent variable \\( Y \\) equals 1.\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (iris)\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # using only two features for simplicity\n",
    "Y = (iris.target != 0) * 1  # making it a binary classification\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = log_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f'Logistic Regression Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "*Real-world Application*: Predicting whether an email is spam or not is a typical application of logistic regression.\n",
    "\n",
    "## Model Evaluation Techniques\n",
    "\n",
    "### Theory\n",
    "\n",
    "To ensure a model's reliability, different model evaluation techniques are employed:\n",
    "\n",
    "1. **Training and Testing Split**: Divides data into training and testing subsets.\n",
    "2. **Cross-validation**: Involves splitting the data into k-folds to ensure all data is used for training and testing in rotations.\n",
    "3. **Metrics**:\n",
    "    - **Mean Squared Error (MSE)**: Evaluates the average of the squared differences between the observed and predicted values.\n",
    "    - **R-squared**: Measures the proportion of variance in the dependent variable predictable from the independent variable(s).\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Example metric calculations\n",
    "cross_val_scores = cross_val_score(linear_model, X, Y, cv=5)\n",
    "\n",
    "# Assuming predictions from the earlier linear regression example\n",
    "mse = mean_squared_error(Y, model.predict(X))\n",
    "r2 = r2_score(Y, model.predict(X))\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "print(f'Cross-Validation Scores: {cross_val_scores}')\n",
    "```\n",
    "\n",
    "*Insight*: These techniques are invaluable in gauging a modelâ€™s performance on unseen data and understanding its generalization capabilities.\n",
    "\n",
    "In conclusion, regression models are indispensable in machine learning, each bringing unique benefits to tackle a variety of challenges across domains. These foundational techniques pave the way for building more sophisticated models tailored to specific needs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
